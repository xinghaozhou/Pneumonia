{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2fb0427",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b2bb562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the Lib\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import seaborn as sns # Use for plot\n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as func\n",
    "import matplotlib.pyplot as plt # Use for plot\n",
    "import sklearn \n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms, models, utils\n",
    "from torchsummary import summary # Visualizing Training Process\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Use for Machine Learning\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "from PIL import Image # Open/Read an Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "a2564dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the Hyperparameters\\\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "1fa73620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Image Transformation\n",
    "image_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(300), # Randomly Crop 300x300 images\n",
    "        # Rotation\n",
    "        # ColorJitter\n",
    "        transforms.RandomHorizontalFlip(), # Horizontally Flip the image\n",
    "        transforms.CenterCrop(256), # Crop 256x256 image from cener\n",
    "        transforms.ToTensor(), # Transform images to Tensor\n",
    "        transforms.Normalize([.485, .456, .406],[.229, .224, .225]) # Normalization for RGB\n",
    "    ]),\n",
    "    \n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(300), # Resize the image to 300x300\n",
    "        transforms.RandomHorizontalFlip(), # Horizontally Flip the image\n",
    "        transforms.ToTensor(), # Transform images to Tensor\n",
    "        transforms.Normalize([.485, .456, .406],[.229, .224, .225])\n",
    "    ]),\n",
    "    \n",
    "    'test':([\n",
    "        transforms.Resize(300), # Resize the image to 300x300\n",
    "        transforms.RandomHorizontalFlip(), # Horizontally Flip the image\n",
    "        transforms.ToTensor(), # Transform images to Tensor\n",
    "        transforms.Normalize([.485, .456, .406],[.229, .224, .225]) # Normalization for RGB\n",
    "    ])\n",
    "}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b3e1ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  4. Load-in Dataset\n",
    "BATCH_SIZE = 128\n",
    "file_dir = '/Users/xinghaozhou/Desktop/DL/chest_xray/'\n",
    "\n",
    "    # 5.1 Train/Val/Test dir \n",
    "train_data_dir = file_dir + 'train/'\n",
    "val_data_dir= file_dir + 'val/'\n",
    "test_data_dir = file_dir + 'test/'\n",
    "\n",
    "    # 5.2 Set train/val/test datasets\n",
    "datasets = {\n",
    "    'train': torchvision.datasets.ImageFolder(train_data_dir, transform = image_transforms['train']),\n",
    "    'val': torchvision.datasets.ImageFolder(val_data_dir, transform = image_transforms['val']),\n",
    "    'test':torchvision.datasets.ImageFolder(test_data_dir, transform = image_transforms['test'])   \n",
    "}\n",
    "\n",
    "    # 5.3 Load in Images for different datsets by the batch_size\n",
    "dataloader = {\n",
    "    'train': DataLoader(datasets['train'], batch_size = BATCH_SIZE, shuffle = True),\n",
    "    'val' : DataLoader(datasets['val'], batch_size = BATCH_SIZE, shuffle = True),\n",
    "    'test': DataLoader(datasets['test'], batch_size = BATCH_SIZE, shuffle = True)    \n",
    "}\n",
    "\n",
    "    # 5.4 Get the Label name and make it as a \"DICT\"\n",
    "LABEL = dict((v, k) for k, v in datasets['train'].class_to_idx.items())\n",
    "LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "b1f542ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPENDIX. Train set info\n",
    "dataloader['train'].dataset\n",
    "dataloader['val'].batch_size\n",
    "dataloader['val'].dataset.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "7fd72263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Normal Images\n",
    "image_normal = os.listdir(os.path.join(dataloader['train'].dataset.root,'NORMAL'))\n",
    "image_pneumonia = os.listdir(os.path.join(dataloader['train'].dataset.root,'Pneumonia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "7a10970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Log writter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = '/Users/xinghaozhou/Desktop/DL/chest_xray/logdir/'\n",
    "\n",
    "def tb_writer():\n",
    "    timestr = time.strftime(\"%Y%m%d__%H%M%S\")\n",
    "    writer = SummaryWriter(log_dir+timestr)\n",
    "    return writer\n",
    "writer = tb_writer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "20df9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Setting for showing images\n",
    "images, label = next(iter(dataloader['train']))\n",
    "def imshow(img):\n",
    "    img = img /2 +0.5 # Unnormalize\n",
    "    npimg = img.numpy() # Tensor -> Numpy\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) # Change the Channel Order Reason\n",
    "    # Reason: plt and Numpy represent 3-dimension in (x,y,z) format whereas PyTorch represents is as (z,x,y) format\n",
    "    plt.show() \n",
    "    \n",
    "grid = utils.make_grid(images) # Make grid that splits the images\n",
    "imshow(grid) # Show the grid\n",
    "\n",
    "writer.add_image('X-Ray Grid: ', grid, 0) # add_image(tag, image_tensor, global_step ...)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2e5ffd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Misclassified Images\n",
    "def misclassified_images(pred, writer, label, images, output, epoch, counts=10):\n",
    "    misclassified = (pred != label.data ) # Determine if it is missclassified\n",
    "    for index, image_tensor in enumerate(images[misclassified][:counts]):\n",
    "        img_name = 'Epoch:{}-->Predict:{}-->Actual:{}'.format(epoch, LABEL[pred[misclassified].tolist()[index]],\n",
    "                                                              LABEL[label.data[misclassified].tolist()[index]])\n",
    "        writer.add_image(img_name, images_tensor, epoch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "aa6b0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Modification of Pooling Layer\n",
    "# Q: Why we have this? A: While training the models, some params might change. We need this to adapt to new params\n",
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    def __init__(self, size = None):\n",
    "        super(AdaptiveConcatPool2d,self).__init__()\n",
    "        size = size or (1,1) # Want it to be 1x1 filter\n",
    "        self.pool_one = nn.AdaptiveAvgPool2d(size) # Modifying 1st Pooling Layer \n",
    "        self.pool_two = nn.AdaptiveMaxPool2d(size) # Modifying 2nd Pooling Layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.pool_one(x), self.pool_two(x)], dim=1) # Concatenate those two new layers seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "eb799772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Transfer Learning\n",
    "def get_model():\n",
    "    model = models.resnet50(pretrained=True) # Using Pre-trained model: Resnet-50\n",
    "    \n",
    "    for param in model.parameters(): \n",
    "        param.requires_grad = False    # Freeze part of the model\n",
    "    \n",
    "    model.avgpool = AdaptiveConcatPool2d() # Modifying AvgPooling Layer to Adaptiive Pooling Layer \n",
    "    model.fc = nn.Sequential( # Container for FC Layer\n",
    "        nn.Flatten(),   # Flattern to 1-dim\n",
    "        nn.BatchNorm1d(4096), # Normalization\n",
    "        nn.Dropout(0.5), # Dropout \n",
    "        nn.Linear(4096,512), # Input: 4096 with 1-dim   Output: 512 with 1-dim\n",
    "        nn.ReLU(), # ReLu Activation Func\n",
    "        nn.BatchNorm1d(512), # Normalizationi\n",
    "        nn.Linear(512,2), # Input: 512 with 1-dim    Output: 2 stands for Pneumonia/Normal\n",
    "        nn.LogSoftmax(dim=1) # Softmax Func to get Prob\n",
    "    )\n",
    "    return model # Return the model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "caaea83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Define the Train model\n",
    "def train_val(model, device, criterion, train_loader, val_loader, optimizer, epoch, writer): # Writer for outputing result into LOG\n",
    "    model.train() # Train model\n",
    "    total_loss = 0.0 # Loss rate\n",
    "    val_loss = 0.0 # Val rate\n",
    "    val_accu = 0.0 # Val accuracy\n",
    "    \n",
    "    for batch_index, (data, label) in enumerate(train_loader): # Iterate over each batch\n",
    "        data, label = data.to(device), label.to(device) # Send data/label to device\n",
    "        optimizer.zero_grad() # Initialize gradient to 0 \n",
    "        output = model(data) # Foward propagation\n",
    "        loss = criterion(data, label) # Calculate the loss rate \n",
    "        loss.backward() # Backpropagation \n",
    "        total_loss += loss.item() * image.size(0) # Accumulate the total_loss \n",
    "        # Reason for why we use image.size(0) -> image.size(0) gives us the current batch_size\n",
    "        # Reason for mul -> since loss.item() gives us the average batch loss \n",
    "        \n",
    "    train_loss = total_loss / len(train_loader.dataset) # Get the average training loss\n",
    "    writer.add_scalar(\"Train loss:\", train_loss, epoch) # Write-in the Average Loss rate into Log\n",
    "    writer.flush() # Write log into the Disk\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, label in val_loader:\n",
    "            data, label = data.to(device), label.to(device) # Send data/label to device\n",
    "            output = model(data) # Foward propagation\n",
    "            loss = criterion(data, label) # Calculate the loss rate \n",
    "            loss.backward() # Backpropagation \n",
    "            val_loss += loss.item() * image.size(0) # Accumulate the total_loss \n",
    "            pred = output.argmax(dim=1) # Pred is the most possible predicition\n",
    "            correct = pred.eq(label.veiw_as(pred)) # Accumulate the correct, return as tensor[True, False, False.....]\n",
    "            # Reason --> Because the batch contains (BATCH_SIZE) images; therefore, the tensor has 128 boolean\n",
    "            accuracy = torch.mean(correct.type(torch.FloatTensor)) # Calculates (Float) (number of \"True\" in tensor/ tensor size)\n",
    "            val_accu += accuracy.item * image.size(0)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accu = val_accu / len(val_loader.dataset)\n",
    "        \n",
    " # Return the Average Loss Rate\n",
    "    return train_loss, val_loss, val_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "701e4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Define the Test model\n",
    "def test_model(model, device, criterion, test_loader, writer, epoch):\n",
    "    model.eval() # Test model\n",
    "    # Initialization \n",
    "    correct = 0.0 \n",
    "    test_loss = 0.0 \n",
    "    \n",
    "    with torch.zero_grad():\n",
    "        for batch_id, (iamges, labels) in enumerate(test_loader):\n",
    "            data, label = data.to(device), label.to(device) # Send data/label to device\n",
    "            output = model(data)  # Foward propagation\n",
    "            loss = criterion(data, label).item() # Calculate the loss rate \n",
    "            test_loss += loss.item() # Accumulate the total_loss \n",
    "            pred = output.argmax(dim=1)  # Pred is the most possible predicition\n",
    "            correct = pred.eq(label.veiw_as(pred)).sum().item() # Get the num that we predict correctly\n",
    "            misclassified_images(pred, writer, label, images, output, epoch, counts=10)\n",
    "            \n",
    "            \n",
    "        avg_loss /= len(test_loader) # Calculate the Average Loss Rate\n",
    "        accuracy = 100 * correct / len(test_loader) # Calculate the Accuracy\n",
    "\n",
    "    writer.add_scalar(\"Test Loss:\", total_loss, epoch)\n",
    "    writer.add_scalar(\"Test Accuracy:\", Accuracy, epoch)\n",
    "    writer.flush()\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "cc7210ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Define the Train process\n",
    "model = get_model().to(DEVICE)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr =0.003)\n",
    "\n",
    "def train_epochs(model, device, dataloader, criterion, epoch, optimizer, writer):\n",
    "    # Output INFO\n",
    "    print(\"{0:>15} | {1:>15} | {2:>15} | {3:>15} | {4:>15} | {5:>15}\".format(\"Epoch\", \"Train Loss\", \"Val Loss\", \"Val Accuracy\", \"Test Loss\", \"Test Accuracy\"))\n",
    "    \n",
    "    # Save the BEST model\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(epoch):\n",
    "        # Train dataset and Return values of \"train_loss\", \"val_loss\", \"val_accu\"\n",
    "        train_loss, val_loss, val_accu = train_val(model, device, criterion, dataloader['train'], dataloader['val'], optimizer, epoch, writer)\n",
    "        # Test dataset and Return values of \"total_loss\" and \"accuracy\"\n",
    "        test_loss, accuracy = test_model(model, device, criterion, dataloader['test'], writer, epoch)\n",
    "        # Update the new best_loss if it is less than the previous one\n",
    "        if best_loss < test_loss:\n",
    "            best_loss = test_loss\n",
    "            # Save the model\n",
    "            torch.save(model.state_dict(), 'model.pth')\n",
    "        # Output the Result\n",
    "        print(\"{0:>15} | {1:>15} | {2:>15} | {3:>15} | {4:>15} | {5:>15}\".format(\"Epoch\", \"Train Loss\", \"Val Loss\", \"Val Accuracy\", \"Test Loss\", \"Test Accuracy\"))\n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "83d0ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model().to(DEVICE) # Get the model\n",
    "criterion = nn.NLLLoss() # Loss Function\n",
    "optimizer = optim.Adam(model.parameters()) # Optimizer Used\n",
    "train_epochs(model, DEVICE, dataloaders, criterion, optimizer, EPOCH, writer)\n",
    "writer.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cee7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
